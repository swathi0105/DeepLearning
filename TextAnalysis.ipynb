{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT ANALYSIS USING BINARY CLASSIFICATION"
      ],
      "metadata": {
        "id": "q5SXS_NdW5AS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Text data of motivational and demotivational quotes\n",
        "2. Preprocessing \n",
        "3. Model creation and evaluation"
      ],
      "metadata": {
        "id": "NF1NdsbbwfBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r14jQtXKNSpJ",
        "outputId": "746fa589-b6e4-4236-f9d3-26e2429cfa7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset Creation:"
      ],
      "metadata": {
        "id": "oz5gsNVzV1uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/text_pdl5.csv\",encoding='cp1252')"
      ],
      "metadata": {
        "id": "8PWgT62kNThR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y50VEJt5bbXF",
        "outputId": "ff22498c-c1ec-4c3f-b9eb-2a55a4bc5d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X    object\n",
              "Y     int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FACPCgT3Ry09",
        "outputId": "9280aca9-afe3-4d5c-d352-951478e300ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   X  Y\n",
              "0  If you want to \\achieve greatness stop asking ...  0\n",
              "1  Success is walking from failure to failure wit...  0\n",
              "2  Trust because you are willing to accept the ri...  0\n",
              "3  Just when the caterpillar thought the world wa...  0\n",
              "4  Whenever you see a successful person you only ...  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06a25378-73c7-44ab-8466-c0b5a58ce4d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If you want to \\achieve greatness stop asking ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Success is walking from failure to failure wit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Trust because you are willing to accept the ri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Just when the caterpillar thought the world wa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Whenever you see a successful person you only ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06a25378-73c7-44ab-8466-c0b5a58ce4d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06a25378-73c7-44ab-8466-c0b5a58ce4d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06a25378-73c7-44ab-8466-c0b5a58ce4d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vmvMg_CFR3ye",
        "outputId": "491e6e95-6769-4007-b1f3-821a8bb1b923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    X  Y\n",
              "36  Never underestimate the power of stupid people...  1\n",
              "37  “Go to heaven for the climate and hell for the...  1\n",
              "38    “I love mankind... it's people I can't stand!”   1\n",
              "39  “I am free of all prejudice. I hate everyone e...  1\n",
              "40  “When we talk to God, we’re praying. When God ...  1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60058ea2-ec08-4a55-a4b6-2c0b78212cb2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Never underestimate the power of stupid people...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>“Go to heaven for the climate and hell for the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>“I love mankind... it's people I can't stand!”</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>“I am free of all prejudice. I hate everyone e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>“When we talk to God, we’re praying. When God ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60058ea2-ec08-4a55-a4b6-2c0b78212cb2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60058ea2-ec08-4a55-a4b6-2c0b78212cb2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60058ea2-ec08-4a55-a4b6-2c0b78212cb2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUkJOtrTR0X7",
        "outputId": "3d739320-dc19-4d9a-ef3f-5caf97e20e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('Y').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "1TbK0sb2R3OS",
        "outputId": "2c107003-b44b-4bdb-d900-0a1e62f2c733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    X\n",
              "Y    \n",
              "0  20\n",
              "1  21"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d603a2c-c932-4aec-b435-f143d7caf6ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Y</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d603a2c-c932-4aec-b435-f143d7caf6ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d603a2c-c932-4aec-b435-f143d7caf6ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d603a2c-c932-4aec-b435-f143d7caf6ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Pre-processing:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RT0gN8juVMDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.X\n",
        "y=df.Y"
      ],
      "metadata": {
        "id": "3Xkbo-j0R95M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "l52gEUK3SRbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_review(review):\n",
        "  tokens = review.lower().split()\n",
        "  filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "  return \" \".join(filtered_tokens)"
      ],
      "metadata": {
        "id": "GIB7dPU4Scki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf0dJC3cSiNN",
        "outputId": "a6d0783f-ce97-475d-bd75-c592361106e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FyJcfgCHW3pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp=X.tolist()\n",
        "fax=[]\n",
        "\n",
        "for i in temp:\n",
        "    fax.append(clean_review(i))\n",
        "n_X=pd.Series(fax)"
      ],
      "metadata": {
        "id": "xdD0ziXCSrSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "vectors = tfidf.fit_transform(n_X)\n",
        "features_name = tfidf.get_feature_names()\n",
        "text_vect = pd.DataFrame(vectors.todense(),columns=features_name)\n",
        "text_vect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0lRXKz2rUKNt",
        "outputId": "32ada204-563d-4993-fc91-5b6c7b034d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         000        10  absolutely    accept   achieve  actually     again  \\\n",
              "0   0.000000  0.000000    0.000000  0.000000  0.421858  0.000000  0.000000   \n",
              "1   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "2   0.000000  0.000000    0.000000  0.421858  0.000000  0.000000  0.000000   \n",
              "3   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "4   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "5   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "6   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "7   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "8   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "9   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "10  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "11  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "12  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "13  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "14  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "15  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "16  0.392348  0.392348    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "17  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "18  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "19  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "20  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "21  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "22  0.000000  0.000000    0.000000  0.000000  0.000000  0.414672  0.000000   \n",
              "23  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "24  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "25  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.294323   \n",
              "26  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "27  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "28  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "29  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "30  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "31  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "32  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "33  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "34  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "35  0.000000  0.000000    0.388688  0.000000  0.000000  0.000000  0.000000   \n",
              "36  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "37  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "38  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "39  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "40  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "       alone    always       ant  ...  whenever   willing      work   working  \\\n",
              "0   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "1   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "2   0.000000  0.000000  0.000000  ...  0.000000  0.379567  0.000000  0.000000   \n",
              "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "4   0.000000  0.000000  0.000000  ...  0.283653  0.000000  0.000000  0.000000   \n",
              "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.582124  0.000000   \n",
              "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "10  0.000000  0.000000  0.000000  ...  0.000000  0.418642  0.000000  0.000000   \n",
              "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "12  0.142261  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.353015  0.000000   \n",
              "17  0.000000  0.751466  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "19  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "20  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "21  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "22  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "23  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "24  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "25  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "26  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "27  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "28  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "29  0.000000  0.000000  0.308663  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "30  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "31  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "32  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "33  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "34  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.353553   \n",
              "35  0.000000  0.349721  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "36  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "37  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "38  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "39  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "40  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "       world     worst     wrong       yet       you  yourself  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "3   0.373294  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "5   0.323526  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "8   0.000000  0.000000  0.395045  0.000000  0.000000  0.000000  \n",
              "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "21  0.000000  0.000000  0.000000  0.000000  0.338813  0.000000  \n",
              "22  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "23  0.000000  0.000000  0.000000  0.557526  0.000000  0.000000  \n",
              "24  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "25  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.439257  \n",
              "27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "29  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "31  0.000000  0.358778  0.000000  0.000000  0.322810  0.000000  \n",
              "32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "33  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "35  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "36  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "37  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "38  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "39  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "40  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "\n",
              "[41 rows x 218 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0b0b098-3298-4a66-9346-aca776e5b010\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>accept</th>\n",
              "      <th>achieve</th>\n",
              "      <th>actually</th>\n",
              "      <th>again</th>\n",
              "      <th>alone</th>\n",
              "      <th>always</th>\n",
              "      <th>ant</th>\n",
              "      <th>...</th>\n",
              "      <th>whenever</th>\n",
              "      <th>willing</th>\n",
              "      <th>work</th>\n",
              "      <th>working</th>\n",
              "      <th>world</th>\n",
              "      <th>worst</th>\n",
              "      <th>wrong</th>\n",
              "      <th>yet</th>\n",
              "      <th>you</th>\n",
              "      <th>yourself</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.421858</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.421858</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.379567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.373294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283653</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.323526</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.582124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.395045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.418642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142261</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.392348</td>\n",
              "      <td>0.392348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.751466</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.338813</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.414672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.557526</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.294323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.439257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.308663</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.358778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.322810</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.388688</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.349721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41 rows × 218 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0b0b098-3298-4a66-9346-aca776e5b010')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0b0b098-3298-4a66-9346-aca776e5b010 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0b0b098-3298-4a66-9346-aca776e5b010');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "temp = tf.Variable(text_vect)"
      ],
      "metadata": {
        "id": "zpNRbEAkUmeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dataset Preparation:"
      ],
      "metadata": {
        "id": "5RH_NM9QU8tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(text_vect,y,train_size=0.75,test_size=0.25)\n"
      ],
      "metadata": {
        "id": "z1Hf3VNbU_ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTdJEmf_Yd7C",
        "outputId": "0735cff1-3bc6-4fb0-f4e1-8e5cd33e2645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 218)\n",
            "(30,)\n",
            "(11, 218)\n",
            "(11,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.layers import Dense,Activation"
      ],
      "metadata": {
        "id": "ueGC_syKYk7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu',input_dim=X_train.shape[1]))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid')) #output layer\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhRWY136YqaJ",
        "outputId": "b63caa6c-a53a-47fe-b6b8-98414c87d174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               28032     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 39,050\n",
            "Trainable params: 39,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history=model.fit(X_train,y_train,epochs=100,verbose=2,validation_split=0.2,batch_size=15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-tacDj5YtPp",
        "outputId": "075455e2-1393-4b29-ff0d-5f9506953131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 - 2s - loss: 0.6935 - accuracy: 0.5833 - val_loss: 0.6895 - val_accuracy: 0.8333 - 2s/epoch - 991ms/step\n",
            "Epoch 2/100\n",
            "2/2 - 0s - loss: 0.6877 - accuracy: 0.6667 - val_loss: 0.6892 - val_accuracy: 0.8333 - 62ms/epoch - 31ms/step\n",
            "Epoch 3/100\n",
            "2/2 - 0s - loss: 0.6834 - accuracy: 0.7917 - val_loss: 0.6889 - val_accuracy: 0.8333 - 68ms/epoch - 34ms/step\n",
            "Epoch 4/100\n",
            "2/2 - 0s - loss: 0.6790 - accuracy: 0.8333 - val_loss: 0.6884 - val_accuracy: 0.8333 - 53ms/epoch - 27ms/step\n",
            "Epoch 5/100\n",
            "2/2 - 0s - loss: 0.6746 - accuracy: 0.8750 - val_loss: 0.6886 - val_accuracy: 0.8333 - 51ms/epoch - 26ms/step\n",
            "Epoch 6/100\n",
            "2/2 - 0s - loss: 0.6683 - accuracy: 0.9583 - val_loss: 0.6889 - val_accuracy: 0.8333 - 51ms/epoch - 25ms/step\n",
            "Epoch 7/100\n",
            "2/2 - 0s - loss: 0.6620 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 0.8333 - 51ms/epoch - 25ms/step\n",
            "Epoch 8/100\n",
            "2/2 - 0s - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 0.8333 - 47ms/epoch - 24ms/step\n",
            "Epoch 9/100\n",
            "2/2 - 0s - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.8333 - 52ms/epoch - 26ms/step\n",
            "Epoch 10/100\n",
            "2/2 - 0s - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.8333 - 59ms/epoch - 30ms/step\n",
            "Epoch 11/100\n",
            "2/2 - 0s - loss: 0.6266 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 0.8333 - 55ms/epoch - 27ms/step\n",
            "Epoch 12/100\n",
            "2/2 - 0s - loss: 0.6139 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.6667 - 54ms/epoch - 27ms/step\n",
            "Epoch 13/100\n",
            "2/2 - 0s - loss: 0.5996 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.6667 - 47ms/epoch - 23ms/step\n",
            "Epoch 14/100\n",
            "2/2 - 0s - loss: 0.5839 - accuracy: 1.0000 - val_loss: 0.6886 - val_accuracy: 0.6667 - 49ms/epoch - 25ms/step\n",
            "Epoch 15/100\n",
            "2/2 - 0s - loss: 0.5640 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.6667 - 50ms/epoch - 25ms/step\n",
            "Epoch 16/100\n",
            "2/2 - 0s - loss: 0.5430 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 17/100\n",
            "2/2 - 0s - loss: 0.5173 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.5000 - 59ms/epoch - 30ms/step\n",
            "Epoch 18/100\n",
            "2/2 - 0s - loss: 0.4880 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.5000 - 50ms/epoch - 25ms/step\n",
            "Epoch 19/100\n",
            "2/2 - 0s - loss: 0.4582 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 0.5000 - 55ms/epoch - 27ms/step\n",
            "Epoch 20/100\n",
            "2/2 - 0s - loss: 0.4227 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 0.5000 - 52ms/epoch - 26ms/step\n",
            "Epoch 21/100\n",
            "2/2 - 0s - loss: 0.3865 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.5000 - 56ms/epoch - 28ms/step\n",
            "Epoch 22/100\n",
            "2/2 - 0s - loss: 0.3472 - accuracy: 1.0000 - val_loss: 0.6908 - val_accuracy: 0.5000 - 50ms/epoch - 25ms/step\n",
            "Epoch 23/100\n",
            "2/2 - 0s - loss: 0.3063 - accuracy: 1.0000 - val_loss: 0.6923 - val_accuracy: 0.5000 - 51ms/epoch - 26ms/step\n",
            "Epoch 24/100\n",
            "2/2 - 0s - loss: 0.2650 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 0.5000 - 51ms/epoch - 26ms/step\n",
            "Epoch 25/100\n",
            "2/2 - 0s - loss: 0.2241 - accuracy: 1.0000 - val_loss: 0.6941 - val_accuracy: 0.5000 - 55ms/epoch - 27ms/step\n",
            "Epoch 26/100\n",
            "2/2 - 0s - loss: 0.1852 - accuracy: 1.0000 - val_loss: 0.6953 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "2/2 - 0s - loss: 0.1493 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 28/100\n",
            "2/2 - 0s - loss: 0.1150 - accuracy: 1.0000 - val_loss: 0.6938 - val_accuracy: 0.6667 - 35ms/epoch - 17ms/step\n",
            "Epoch 29/100\n",
            "2/2 - 0s - loss: 0.0861 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.6667 - 34ms/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "2/2 - 0s - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.6811 - val_accuracy: 0.6667 - 33ms/epoch - 16ms/step\n",
            "Epoch 31/100\n",
            "2/2 - 0s - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.6768 - val_accuracy: 0.6667 - 43ms/epoch - 22ms/step\n",
            "Epoch 32/100\n",
            "2/2 - 0s - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.6733 - val_accuracy: 0.5000 - 37ms/epoch - 18ms/step\n",
            "Epoch 33/100\n",
            "2/2 - 0s - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.6684 - val_accuracy: 0.5000 - 43ms/epoch - 21ms/step\n",
            "Epoch 34/100\n",
            "2/2 - 0s - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 35/100\n",
            "2/2 - 0s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.6590 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 36/100\n",
            "2/2 - 0s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 42ms/epoch - 21ms/step\n",
            "Epoch 37/100\n",
            "2/2 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 38/100\n",
            "2/2 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.6532 - val_accuracy: 0.5000 - 39ms/epoch - 19ms/step\n",
            "Epoch 39/100\n",
            "2/2 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6530 - val_accuracy: 0.5000 - 44ms/epoch - 22ms/step\n",
            "Epoch 40/100\n",
            "2/2 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 0.5000 - 41ms/epoch - 21ms/step\n",
            "Epoch 41/100\n",
            "2/2 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 0.5000 - 41ms/epoch - 20ms/step\n",
            "Epoch 42/100\n",
            "2/2 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.5000 - 40ms/epoch - 20ms/step\n",
            "Epoch 43/100\n",
            "2/2 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.5000 - 40ms/epoch - 20ms/step\n",
            "Epoch 44/100\n",
            "2/2 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 0.5000 - 35ms/epoch - 18ms/step\n",
            "Epoch 45/100\n",
            "2/2 - 0s - loss: 9.9959e-04 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 46/100\n",
            "2/2 - 0s - loss: 8.9044e-04 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 47/100\n",
            "2/2 - 0s - loss: 8.0531e-04 - accuracy: 1.0000 - val_loss: 0.6530 - val_accuracy: 0.5000 - 35ms/epoch - 18ms/step\n",
            "Epoch 48/100\n",
            "2/2 - 0s - loss: 7.3283e-04 - accuracy: 1.0000 - val_loss: 0.6532 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 49/100\n",
            "2/2 - 0s - loss: 6.7926e-04 - accuracy: 1.0000 - val_loss: 0.6533 - val_accuracy: 0.5000 - 39ms/epoch - 20ms/step\n",
            "Epoch 50/100\n",
            "2/2 - 0s - loss: 6.3104e-04 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 0.5000 - 40ms/epoch - 20ms/step\n",
            "Epoch 51/100\n",
            "2/2 - 0s - loss: 5.9367e-04 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 0.5000 - 35ms/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "2/2 - 0s - loss: 5.6009e-04 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 0.5000 - 39ms/epoch - 20ms/step\n",
            "Epoch 53/100\n",
            "2/2 - 0s - loss: 5.3254e-04 - accuracy: 1.0000 - val_loss: 0.6539 - val_accuracy: 0.5000 - 39ms/epoch - 19ms/step\n",
            "Epoch 54/100\n",
            "2/2 - 0s - loss: 5.0850e-04 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "2/2 - 0s - loss: 4.8601e-04 - accuracy: 1.0000 - val_loss: 0.6542 - val_accuracy: 0.5000 - 67ms/epoch - 33ms/step\n",
            "Epoch 56/100\n",
            "2/2 - 0s - loss: 4.6747e-04 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 0.5000 - 37ms/epoch - 19ms/step\n",
            "Epoch 57/100\n",
            "2/2 - 0s - loss: 4.5118e-04 - accuracy: 1.0000 - val_loss: 0.6545 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 58/100\n",
            "2/2 - 0s - loss: 4.3495e-04 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 59/100\n",
            "2/2 - 0s - loss: 4.2103e-04 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 60/100\n",
            "2/2 - 0s - loss: 4.0749e-04 - accuracy: 1.0000 - val_loss: 0.6550 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 61/100\n",
            "2/2 - 0s - loss: 3.9688e-04 - accuracy: 1.0000 - val_loss: 0.6552 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 62/100\n",
            "2/2 - 0s - loss: 3.8555e-04 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 0.5000 - 44ms/epoch - 22ms/step\n",
            "Epoch 63/100\n",
            "2/2 - 0s - loss: 3.7582e-04 - accuracy: 1.0000 - val_loss: 0.6554 - val_accuracy: 0.5000 - 35ms/epoch - 18ms/step\n",
            "Epoch 64/100\n",
            "2/2 - 0s - loss: 3.6563e-04 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 65/100\n",
            "2/2 - 0s - loss: 3.5645e-04 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 66/100\n",
            "2/2 - 0s - loss: 3.4837e-04 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 67/100\n",
            "2/2 - 0s - loss: 3.4047e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 41ms/epoch - 20ms/step\n",
            "Epoch 68/100\n",
            "2/2 - 0s - loss: 3.3337e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 69/100\n",
            "2/2 - 0s - loss: 3.2500e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 39ms/epoch - 20ms/step\n",
            "Epoch 70/100\n",
            "2/2 - 0s - loss: 3.1889e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 37ms/epoch - 18ms/step\n",
            "Epoch 71/100\n",
            "2/2 - 0s - loss: 3.1173e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 43ms/epoch - 22ms/step\n",
            "Epoch 72/100\n",
            "2/2 - 0s - loss: 3.0534e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "2/2 - 0s - loss: 2.9949e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.5000 - 37ms/epoch - 19ms/step\n",
            "Epoch 74/100\n",
            "2/2 - 0s - loss: 2.9352e-04 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.5000 - 39ms/epoch - 20ms/step\n",
            "Epoch 75/100\n",
            "2/2 - 0s - loss: 2.8770e-04 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 76/100\n",
            "2/2 - 0s - loss: 2.8228e-04 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 77/100\n",
            "2/2 - 0s - loss: 2.7675e-04 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.5000 - 32ms/epoch - 16ms/step\n",
            "Epoch 78/100\n",
            "2/2 - 0s - loss: 2.7187e-04 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.5000 - 32ms/epoch - 16ms/step\n",
            "Epoch 79/100\n",
            "2/2 - 0s - loss: 2.6653e-04 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.5000 - 41ms/epoch - 20ms/step\n",
            "Epoch 80/100\n",
            "2/2 - 0s - loss: 2.6208e-04 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 81/100\n",
            "2/2 - 0s - loss: 2.5693e-04 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 82/100\n",
            "2/2 - 0s - loss: 2.5238e-04 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.5000 - 35ms/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "2/2 - 0s - loss: 2.4839e-04 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.5000 - 49ms/epoch - 24ms/step\n",
            "Epoch 84/100\n",
            "2/2 - 0s - loss: 2.4359e-04 - accuracy: 1.0000 - val_loss: 0.6560 - val_accuracy: 0.5000 - 37ms/epoch - 18ms/step\n",
            "Epoch 85/100\n",
            "2/2 - 0s - loss: 2.3931e-04 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 0.5000 - 31ms/epoch - 16ms/step\n",
            "Epoch 86/100\n",
            "2/2 - 0s - loss: 2.3512e-04 - accuracy: 1.0000 - val_loss: 0.6562 - val_accuracy: 0.5000 - 37ms/epoch - 19ms/step\n",
            "Epoch 87/100\n",
            "2/2 - 0s - loss: 2.3117e-04 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 88/100\n",
            "2/2 - 0s - loss: 2.2713e-04 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 32ms/epoch - 16ms/step\n",
            "Epoch 89/100\n",
            "2/2 - 0s - loss: 2.2315e-04 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "2/2 - 0s - loss: 2.1930e-04 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 91/100\n",
            "2/2 - 0s - loss: 2.1545e-04 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 44ms/epoch - 22ms/step\n",
            "Epoch 92/100\n",
            "2/2 - 0s - loss: 2.1195e-04 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 93/100\n",
            "2/2 - 0s - loss: 2.0827e-04 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 0.5000 - 39ms/epoch - 20ms/step\n",
            "Epoch 94/100\n",
            "2/2 - 0s - loss: 2.0481e-04 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 0.5000 - 35ms/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "2/2 - 0s - loss: 2.0123e-04 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "2/2 - 0s - loss: 1.9808e-04 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 0.5000 - 39ms/epoch - 19ms/step\n",
            "Epoch 97/100\n",
            "2/2 - 0s - loss: 1.9451e-04 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 0.5000 - 32ms/epoch - 16ms/step\n",
            "Epoch 98/100\n",
            "2/2 - 0s - loss: 1.9165e-04 - accuracy: 1.0000 - val_loss: 0.6568 - val_accuracy: 0.5000 - 33ms/epoch - 16ms/step\n",
            "Epoch 99/100\n",
            "2/2 - 0s - loss: 1.8823e-04 - accuracy: 1.0000 - val_loss: 0.6569 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 100/100\n",
            "2/2 - 0s - loss: 1.8528e-04 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 0.5000 - 35ms/epoch - 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVXu1O9GY8Vi",
        "outputId": "0eb4081b-2b01-4f93-ca96-dffd76be1077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1323 - accuracy: 0.5455\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1322685480117798, 0.5454545617103577]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "SKpYARaXZAs6",
        "outputId": "c49d5554-5e85-4424-f0e4-60ae8aad372d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(512, activation='relu',input_dim=X_train.shape[1]))\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dense(2, activation='sigmoid'))\n",
        "model2.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv56L6y_ZLGc",
        "outputId": "588d63a1-65ba-4992-fffd-81dfbf64fc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 512)               112128    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 284,738\n",
            "Trainable params: 284,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history=model2.fit(X_train,y_train,epochs=100,verbose=2,validation_split=0.2,batch_size=15)"
      ],
      "metadata": {
        "id": "9zUheBY8bALh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586a9b72-4186-46ca-983f-0476a72a2bf1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 - 2s - loss: 0.6995 - accuracy: 0.4167 - val_loss: 0.6886 - val_accuracy: 0.8333 - 2s/epoch - 1s/step\n",
            "Epoch 2/100\n",
            "2/2 - 0s - loss: 0.6571 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.8333 - 102ms/epoch - 51ms/step\n",
            "Epoch 3/100\n",
            "2/2 - 0s - loss: 0.6159 - accuracy: 1.0000 - val_loss: 0.6856 - val_accuracy: 0.6667 - 85ms/epoch - 42ms/step\n",
            "Epoch 4/100\n",
            "2/2 - 0s - loss: 0.5671 - accuracy: 1.0000 - val_loss: 0.6901 - val_accuracy: 0.3333 - 52ms/epoch - 26ms/step\n",
            "Epoch 5/100\n",
            "2/2 - 0s - loss: 0.5028 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.3333 - 56ms/epoch - 28ms/step\n",
            "Epoch 6/100\n",
            "2/2 - 0s - loss: 0.4201 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 7/100\n",
            "2/2 - 0s - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.7013 - val_accuracy: 0.3333 - 109ms/epoch - 55ms/step\n",
            "Epoch 8/100\n",
            "2/2 - 0s - loss: 0.2248 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.5000 - 82ms/epoch - 41ms/step\n",
            "Epoch 9/100\n",
            "2/2 - 0s - loss: 0.1339 - accuracy: 1.0000 - val_loss: 0.7217 - val_accuracy: 0.3333 - 78ms/epoch - 39ms/step\n",
            "Epoch 10/100\n",
            "2/2 - 0s - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.7416 - val_accuracy: 0.3333 - 65ms/epoch - 33ms/step\n",
            "Epoch 11/100\n",
            "2/2 - 0s - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.7629 - val_accuracy: 0.3333 - 86ms/epoch - 43ms/step\n",
            "Epoch 12/100\n",
            "2/2 - 0s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.7899 - val_accuracy: 0.3333 - 85ms/epoch - 43ms/step\n",
            "Epoch 13/100\n",
            "2/2 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.8205 - val_accuracy: 0.3333 - 106ms/epoch - 53ms/step\n",
            "Epoch 14/100\n",
            "2/2 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8495 - val_accuracy: 0.3333 - 77ms/epoch - 38ms/step\n",
            "Epoch 15/100\n",
            "2/2 - 0s - loss: 5.0367e-04 - accuracy: 1.0000 - val_loss: 0.8794 - val_accuracy: 0.3333 - 60ms/epoch - 30ms/step\n",
            "Epoch 16/100\n",
            "2/2 - 0s - loss: 2.1023e-04 - accuracy: 1.0000 - val_loss: 0.9067 - val_accuracy: 0.3333 - 107ms/epoch - 54ms/step\n",
            "Epoch 17/100\n",
            "2/2 - 0s - loss: 9.7642e-05 - accuracy: 1.0000 - val_loss: 0.9314 - val_accuracy: 0.3333 - 76ms/epoch - 38ms/step\n",
            "Epoch 18/100\n",
            "2/2 - 0s - loss: 4.7443e-05 - accuracy: 1.0000 - val_loss: 0.9530 - val_accuracy: 0.3333 - 54ms/epoch - 27ms/step\n",
            "Epoch 19/100\n",
            "2/2 - 0s - loss: 2.7109e-05 - accuracy: 1.0000 - val_loss: 0.9715 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 20/100\n",
            "2/2 - 0s - loss: 1.5641e-05 - accuracy: 1.0000 - val_loss: 0.9876 - val_accuracy: 0.3333 - 65ms/epoch - 32ms/step\n",
            "Epoch 21/100\n",
            "2/2 - 0s - loss: 1.0530e-05 - accuracy: 1.0000 - val_loss: 1.0014 - val_accuracy: 0.3333 - 86ms/epoch - 43ms/step\n",
            "Epoch 22/100\n",
            "2/2 - 0s - loss: 7.3363e-06 - accuracy: 1.0000 - val_loss: 1.0130 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 23/100\n",
            "2/2 - 0s - loss: 5.1607e-06 - accuracy: 1.0000 - val_loss: 1.0231 - val_accuracy: 0.3333 - 100ms/epoch - 50ms/step\n",
            "Epoch 24/100\n",
            "2/2 - 0s - loss: 3.8991e-06 - accuracy: 1.0000 - val_loss: 1.0317 - val_accuracy: 0.3333 - 61ms/epoch - 30ms/step\n",
            "Epoch 25/100\n",
            "2/2 - 0s - loss: 3.1789e-06 - accuracy: 1.0000 - val_loss: 1.0388 - val_accuracy: 0.3333 - 55ms/epoch - 27ms/step\n",
            "Epoch 26/100\n",
            "2/2 - 0s - loss: 2.5431e-06 - accuracy: 1.0000 - val_loss: 1.0447 - val_accuracy: 0.3333 - 72ms/epoch - 36ms/step\n",
            "Epoch 27/100\n",
            "2/2 - 0s - loss: 2.2103e-06 - accuracy: 1.0000 - val_loss: 1.0497 - val_accuracy: 0.3333 - 95ms/epoch - 48ms/step\n",
            "Epoch 28/100\n",
            "2/2 - 0s - loss: 1.8974e-06 - accuracy: 1.0000 - val_loss: 1.0539 - val_accuracy: 0.3333 - 85ms/epoch - 43ms/step\n",
            "Epoch 29/100\n",
            "2/2 - 0s - loss: 1.7335e-06 - accuracy: 1.0000 - val_loss: 1.0574 - val_accuracy: 0.3333 - 53ms/epoch - 26ms/step\n",
            "Epoch 30/100\n",
            "2/2 - 0s - loss: 1.5746e-06 - accuracy: 1.0000 - val_loss: 1.0603 - val_accuracy: 0.3333 - 51ms/epoch - 26ms/step\n",
            "Epoch 31/100\n",
            "2/2 - 0s - loss: 1.4603e-06 - accuracy: 1.0000 - val_loss: 1.0627 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 32/100\n",
            "2/2 - 0s - loss: 1.3610e-06 - accuracy: 1.0000 - val_loss: 1.0647 - val_accuracy: 0.3333 - 71ms/epoch - 35ms/step\n",
            "Epoch 33/100\n",
            "2/2 - 0s - loss: 1.2865e-06 - accuracy: 1.0000 - val_loss: 1.0663 - val_accuracy: 0.3333 - 73ms/epoch - 37ms/step\n",
            "Epoch 34/100\n",
            "2/2 - 0s - loss: 1.2418e-06 - accuracy: 1.0000 - val_loss: 1.0677 - val_accuracy: 0.3333 - 109ms/epoch - 55ms/step\n",
            "Epoch 35/100\n",
            "2/2 - 0s - loss: 1.1871e-06 - accuracy: 1.0000 - val_loss: 1.0688 - val_accuracy: 0.3333 - 118ms/epoch - 59ms/step\n",
            "Epoch 36/100\n",
            "2/2 - 0s - loss: 1.1573e-06 - accuracy: 1.0000 - val_loss: 1.0697 - val_accuracy: 0.3333 - 66ms/epoch - 33ms/step\n",
            "Epoch 37/100\n",
            "2/2 - 0s - loss: 1.1375e-06 - accuracy: 1.0000 - val_loss: 1.0704 - val_accuracy: 0.3333 - 73ms/epoch - 37ms/step\n",
            "Epoch 38/100\n",
            "2/2 - 0s - loss: 1.1126e-06 - accuracy: 1.0000 - val_loss: 1.0710 - val_accuracy: 0.3333 - 87ms/epoch - 44ms/step\n",
            "Epoch 39/100\n",
            "2/2 - 0s - loss: 1.0878e-06 - accuracy: 1.0000 - val_loss: 1.0715 - val_accuracy: 0.3333 - 61ms/epoch - 30ms/step\n",
            "Epoch 40/100\n",
            "2/2 - 0s - loss: 1.0629e-06 - accuracy: 1.0000 - val_loss: 1.0719 - val_accuracy: 0.3333 - 71ms/epoch - 35ms/step\n",
            "Epoch 41/100\n",
            "2/2 - 0s - loss: 1.0530e-06 - accuracy: 1.0000 - val_loss: 1.0723 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 42/100\n",
            "2/2 - 0s - loss: 1.0282e-06 - accuracy: 1.0000 - val_loss: 1.0725 - val_accuracy: 0.3333 - 65ms/epoch - 33ms/step\n",
            "Epoch 43/100\n",
            "2/2 - 0s - loss: 1.0282e-06 - accuracy: 1.0000 - val_loss: 1.0728 - val_accuracy: 0.3333 - 61ms/epoch - 31ms/step\n",
            "Epoch 44/100\n",
            "2/2 - 0s - loss: 1.0133e-06 - accuracy: 1.0000 - val_loss: 1.0729 - val_accuracy: 0.3333 - 56ms/epoch - 28ms/step\n",
            "Epoch 45/100\n",
            "2/2 - 0s - loss: 1.0083e-06 - accuracy: 1.0000 - val_loss: 1.0731 - val_accuracy: 0.3333 - 59ms/epoch - 29ms/step\n",
            "Epoch 46/100\n",
            "2/2 - 0s - loss: 1.0033e-06 - accuracy: 1.0000 - val_loss: 1.0732 - val_accuracy: 0.3333 - 74ms/epoch - 37ms/step\n",
            "Epoch 47/100\n",
            "2/2 - 0s - loss: 9.9838e-07 - accuracy: 1.0000 - val_loss: 1.0733 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 48/100\n",
            "2/2 - 0s - loss: 9.9838e-07 - accuracy: 1.0000 - val_loss: 1.0734 - val_accuracy: 0.3333 - 64ms/epoch - 32ms/step\n",
            "Epoch 49/100\n",
            "2/2 - 0s - loss: 9.9341e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 55ms/epoch - 28ms/step\n",
            "Epoch 50/100\n",
            "2/2 - 0s - loss: 9.8844e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 100ms/epoch - 50ms/step\n",
            "Epoch 51/100\n",
            "2/2 - 0s - loss: 9.8348e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 142ms/epoch - 71ms/step\n",
            "Epoch 52/100\n",
            "2/2 - 0s - loss: 9.7354e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 63ms/epoch - 31ms/step\n",
            "Epoch 53/100\n",
            "2/2 - 0s - loss: 9.7354e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 53ms/epoch - 27ms/step\n",
            "Epoch 54/100\n",
            "2/2 - 0s - loss: 9.7354e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 61ms/epoch - 30ms/step\n",
            "Epoch 55/100\n",
            "2/2 - 0s - loss: 9.7354e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 61ms/epoch - 30ms/step\n",
            "Epoch 56/100\n",
            "2/2 - 0s - loss: 9.7354e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 55ms/epoch - 27ms/step\n",
            "Epoch 57/100\n",
            "2/2 - 0s - loss: 9.6857e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 62ms/epoch - 31ms/step\n",
            "Epoch 58/100\n",
            "2/2 - 0s - loss: 9.6361e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 59ms/epoch - 29ms/step\n",
            "Epoch 59/100\n",
            "2/2 - 0s - loss: 9.5864e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 60ms/epoch - 30ms/step\n",
            "Epoch 60/100\n",
            "2/2 - 0s - loss: 9.5864e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 78ms/epoch - 39ms/step\n",
            "Epoch 61/100\n",
            "2/2 - 0s - loss: 9.5864e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 82ms/epoch - 41ms/step\n",
            "Epoch 62/100\n",
            "2/2 - 0s - loss: 9.4871e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 89ms/epoch - 45ms/step\n",
            "Epoch 63/100\n",
            "2/2 - 0s - loss: 9.4871e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 73ms/epoch - 37ms/step\n",
            "Epoch 64/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 58ms/epoch - 29ms/step\n",
            "Epoch 65/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 66ms/epoch - 33ms/step\n",
            "Epoch 66/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 54ms/epoch - 27ms/step\n",
            "Epoch 67/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 48ms/epoch - 24ms/step\n",
            "Epoch 68/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 43ms/epoch - 22ms/step\n",
            "Epoch 69/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 70/100\n",
            "2/2 - 0s - loss: 9.3380e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 43ms/epoch - 22ms/step\n",
            "Epoch 71/100\n",
            "2/2 - 0s - loss: 9.2884e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 44ms/epoch - 22ms/step\n",
            "Epoch 72/100\n",
            "2/2 - 0s - loss: 9.2884e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 41ms/epoch - 21ms/step\n",
            "Epoch 73/100\n",
            "2/2 - 0s - loss: 9.2387e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 40ms/epoch - 20ms/step\n",
            "Epoch 74/100\n",
            "2/2 - 0s - loss: 9.1394e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 39ms/epoch - 19ms/step\n",
            "Epoch 75/100\n",
            "2/2 - 0s - loss: 9.1394e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 43ms/epoch - 21ms/step\n",
            "Epoch 76/100\n",
            "2/2 - 0s - loss: 9.1394e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 77/100\n",
            "2/2 - 0s - loss: 9.1394e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 39ms/epoch - 20ms/step\n",
            "Epoch 78/100\n",
            "2/2 - 0s - loss: 9.0897e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 43ms/epoch - 21ms/step\n",
            "Epoch 79/100\n",
            "2/2 - 0s - loss: 9.0897e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 39ms/epoch - 20ms/step\n",
            "Epoch 80/100\n",
            "2/2 - 0s - loss: 9.0897e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n",
            "Epoch 81/100\n",
            "2/2 - 0s - loss: 9.0897e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 41ms/epoch - 20ms/step\n",
            "Epoch 82/100\n",
            "2/2 - 0s - loss: 9.0400e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 43ms/epoch - 22ms/step\n",
            "Epoch 83/100\n",
            "2/2 - 0s - loss: 9.0400e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 37ms/epoch - 18ms/step\n",
            "Epoch 84/100\n",
            "2/2 - 0s - loss: 8.9904e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 37ms/epoch - 18ms/step\n",
            "Epoch 85/100\n",
            "2/2 - 0s - loss: 8.9904e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 39ms/epoch - 20ms/step\n",
            "Epoch 86/100\n",
            "2/2 - 0s - loss: 8.9904e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 35ms/epoch - 17ms/step\n",
            "Epoch 87/100\n",
            "2/2 - 0s - loss: 8.9407e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 42ms/epoch - 21ms/step\n",
            "Epoch 88/100\n",
            "2/2 - 0s - loss: 8.9407e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n",
            "Epoch 89/100\n",
            "2/2 - 0s - loss: 8.9407e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 40ms/epoch - 20ms/step\n",
            "Epoch 90/100\n",
            "2/2 - 0s - loss: 8.9407e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 44ms/epoch - 22ms/step\n",
            "Epoch 91/100\n",
            "2/2 - 0s - loss: 8.9407e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n",
            "Epoch 92/100\n",
            "2/2 - 0s - loss: 8.8910e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 40ms/epoch - 20ms/step\n",
            "Epoch 93/100\n",
            "2/2 - 0s - loss: 8.8910e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 35ms/epoch - 18ms/step\n",
            "Epoch 94/100\n",
            "2/2 - 0s - loss: 8.8413e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 35ms/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "2/2 - 0s - loss: 8.8413e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 37ms/epoch - 18ms/step\n",
            "Epoch 96/100\n",
            "2/2 - 0s - loss: 8.7917e-07 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.3333 - 38ms/epoch - 19ms/step\n",
            "Epoch 97/100\n",
            "2/2 - 0s - loss: 8.7420e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 38ms/epoch - 19ms/step\n",
            "Epoch 98/100\n",
            "2/2 - 0s - loss: 8.6923e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 41ms/epoch - 20ms/step\n",
            "Epoch 99/100\n",
            "2/2 - 0s - loss: 8.6427e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 44ms/epoch - 22ms/step\n",
            "Epoch 100/100\n",
            "2/2 - 0s - loss: 8.6427e-07 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKnbK_uIuu_X",
        "outputId": "e6b84a7c-5f33-4b9a-d778-07e081912dfd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step - loss: 1.6802 - accuracy: 0.4545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6802157163619995, 0.4545454680919647]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "uExu7Rqju9CZ",
        "outputId": "45a1f9dd-c763-4650-f582-8dcdc8e42928"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with two hidden layer"
      ],
      "metadata": {
        "id": "4-niBNQZvCtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Dense(1028, activation='relu',input_dim=X_train.shape[1]))\n",
        "model3.add(Dense(512, activation='relu'))\n",
        "model3.add(Dense(256, activation='relu'))\n",
        "model3.add(Dense(2, activation='sigmoid')) #output layer\n",
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA8jAEzGu_3Z",
        "outputId": "e25ee0c3-2d22-4fea-bedf-fc0495969e3a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 1028)              225132    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               526848    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 883,822\n",
            "Trainable params: 883,822\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history=model3.fit(X_train,y_train,epochs=100,verbose=2,validation_split=0.2,batch_size=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a14nUsoWhoIN",
        "outputId": "48adc790-2dc6-49a0-8a28-2beb4bccc918"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 - 1s - loss: 0.6918 - accuracy: 0.5000 - val_loss: 0.6996 - val_accuracy: 0.5000 - 1s/epoch - 513ms/step\n",
            "Epoch 2/100\n",
            "2/2 - 0s - loss: 0.6110 - accuracy: 0.9583 - val_loss: 0.7079 - val_accuracy: 0.5000 - 74ms/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "2/2 - 0s - loss: 0.5253 - accuracy: 1.0000 - val_loss: 0.7193 - val_accuracy: 0.5000 - 73ms/epoch - 37ms/step\n",
            "Epoch 4/100\n",
            "2/2 - 0s - loss: 0.4127 - accuracy: 1.0000 - val_loss: 0.7318 - val_accuracy: 0.5000 - 81ms/epoch - 41ms/step\n",
            "Epoch 5/100\n",
            "2/2 - 0s - loss: 0.2773 - accuracy: 1.0000 - val_loss: 0.7734 - val_accuracy: 0.5000 - 73ms/epoch - 37ms/step\n",
            "Epoch 6/100\n",
            "2/2 - 0s - loss: 0.1465 - accuracy: 1.0000 - val_loss: 0.8330 - val_accuracy: 0.5000 - 81ms/epoch - 41ms/step\n",
            "Epoch 7/100\n",
            "2/2 - 0s - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.8929 - val_accuracy: 0.6667 - 70ms/epoch - 35ms/step\n",
            "Epoch 8/100\n",
            "2/2 - 0s - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.9464 - val_accuracy: 0.5000 - 84ms/epoch - 42ms/step\n",
            "Epoch 9/100\n",
            "2/2 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.0028 - val_accuracy: 0.5000 - 70ms/epoch - 35ms/step\n",
            "Epoch 10/100\n",
            "2/2 - 0s - loss: 8.2710e-04 - accuracy: 1.0000 - val_loss: 1.0561 - val_accuracy: 0.5000 - 79ms/epoch - 39ms/step\n",
            "Epoch 11/100\n",
            "2/2 - 0s - loss: 1.8080e-04 - accuracy: 1.0000 - val_loss: 1.1071 - val_accuracy: 0.5000 - 74ms/epoch - 37ms/step\n",
            "Epoch 12/100\n",
            "2/2 - 0s - loss: 5.0417e-05 - accuracy: 1.0000 - val_loss: 1.1546 - val_accuracy: 0.5000 - 67ms/epoch - 34ms/step\n",
            "Epoch 13/100\n",
            "2/2 - 0s - loss: 1.3306e-05 - accuracy: 1.0000 - val_loss: 1.1984 - val_accuracy: 0.5000 - 70ms/epoch - 35ms/step\n",
            "Epoch 14/100\n",
            "2/2 - 0s - loss: 4.1375e-06 - accuracy: 1.0000 - val_loss: 1.2387 - val_accuracy: 0.5000 - 81ms/epoch - 40ms/step\n",
            "Epoch 15/100\n",
            "2/2 - 0s - loss: 1.5746e-06 - accuracy: 1.0000 - val_loss: 1.2756 - val_accuracy: 0.5000 - 78ms/epoch - 39ms/step\n",
            "Epoch 16/100\n",
            "2/2 - 0s - loss: 7.8479e-07 - accuracy: 1.0000 - val_loss: 1.3086 - val_accuracy: 0.5000 - 82ms/epoch - 41ms/step\n",
            "Epoch 17/100\n",
            "2/2 - 0s - loss: 3.8246e-07 - accuracy: 1.0000 - val_loss: 1.3379 - val_accuracy: 0.5000 - 77ms/epoch - 38ms/step\n",
            "Epoch 18/100\n",
            "2/2 - 0s - loss: 1.8875e-07 - accuracy: 1.0000 - val_loss: 1.3636 - val_accuracy: 0.5000 - 105ms/epoch - 52ms/step\n",
            "Epoch 19/100\n",
            "2/2 - 0s - loss: 1.1921e-07 - accuracy: 1.0000 - val_loss: 1.3859 - val_accuracy: 0.5000 - 72ms/epoch - 36ms/step\n",
            "Epoch 20/100\n",
            "2/2 - 0s - loss: 6.4572e-08 - accuracy: 1.0000 - val_loss: 1.4050 - val_accuracy: 0.6667 - 77ms/epoch - 39ms/step\n",
            "Epoch 21/100\n",
            "2/2 - 0s - loss: 4.4703e-08 - accuracy: 1.0000 - val_loss: 1.4214 - val_accuracy: 0.6667 - 67ms/epoch - 33ms/step\n",
            "Epoch 22/100\n",
            "2/2 - 0s - loss: 2.9802e-08 - accuracy: 1.0000 - val_loss: 1.4352 - val_accuracy: 0.6667 - 67ms/epoch - 34ms/step\n",
            "Epoch 23/100\n",
            "2/2 - 0s - loss: 2.4835e-08 - accuracy: 1.0000 - val_loss: 1.4469 - val_accuracy: 0.6667 - 76ms/epoch - 38ms/step\n",
            "Epoch 24/100\n",
            "2/2 - 0s - loss: 2.4835e-08 - accuracy: 1.0000 - val_loss: 1.4567 - val_accuracy: 0.6667 - 66ms/epoch - 33ms/step\n",
            "Epoch 25/100\n",
            "2/2 - 0s - loss: 2.4835e-08 - accuracy: 1.0000 - val_loss: 1.4650 - val_accuracy: 0.6667 - 72ms/epoch - 36ms/step\n",
            "Epoch 26/100\n",
            "2/2 - 0s - loss: 1.4901e-08 - accuracy: 1.0000 - val_loss: 1.4719 - val_accuracy: 0.6667 - 66ms/epoch - 33ms/step\n",
            "Epoch 27/100\n",
            "2/2 - 0s - loss: 1.4901e-08 - accuracy: 1.0000 - val_loss: 1.4776 - val_accuracy: 0.6667 - 75ms/epoch - 37ms/step\n",
            "Epoch 28/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4824 - val_accuracy: 0.6667 - 81ms/epoch - 40ms/step\n",
            "Epoch 29/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4863 - val_accuracy: 0.6667 - 67ms/epoch - 33ms/step\n",
            "Epoch 30/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4896 - val_accuracy: 0.6667 - 76ms/epoch - 38ms/step\n",
            "Epoch 31/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4923 - val_accuracy: 0.6667 - 102ms/epoch - 51ms/step\n",
            "Epoch 32/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4946 - val_accuracy: 0.6667 - 76ms/epoch - 38ms/step\n",
            "Epoch 33/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4964 - val_accuracy: 0.6667 - 85ms/epoch - 43ms/step\n",
            "Epoch 34/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4979 - val_accuracy: 0.6667 - 66ms/epoch - 33ms/step\n",
            "Epoch 35/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.4992 - val_accuracy: 0.6667 - 80ms/epoch - 40ms/step\n",
            "Epoch 36/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5002 - val_accuracy: 0.6667 - 85ms/epoch - 43ms/step\n",
            "Epoch 37/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5011 - val_accuracy: 0.6667 - 66ms/epoch - 33ms/step\n",
            "Epoch 38/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5018 - val_accuracy: 0.6667 - 68ms/epoch - 34ms/step\n",
            "Epoch 39/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5023 - val_accuracy: 0.6667 - 67ms/epoch - 34ms/step\n",
            "Epoch 40/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5028 - val_accuracy: 0.6667 - 67ms/epoch - 33ms/step\n",
            "Epoch 41/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5032 - val_accuracy: 0.6667 - 88ms/epoch - 44ms/step\n",
            "Epoch 42/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5035 - val_accuracy: 0.6667 - 70ms/epoch - 35ms/step\n",
            "Epoch 43/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5038 - val_accuracy: 0.6667 - 63ms/epoch - 31ms/step\n",
            "Epoch 44/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5040 - val_accuracy: 0.6667 - 70ms/epoch - 35ms/step\n",
            "Epoch 45/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5042 - val_accuracy: 0.6667 - 87ms/epoch - 43ms/step\n",
            "Epoch 46/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5043 - val_accuracy: 0.6667 - 74ms/epoch - 37ms/step\n",
            "Epoch 47/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5044 - val_accuracy: 0.6667 - 74ms/epoch - 37ms/step\n",
            "Epoch 48/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5045 - val_accuracy: 0.6667 - 78ms/epoch - 39ms/step\n",
            "Epoch 49/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5046 - val_accuracy: 0.6667 - 94ms/epoch - 47ms/step\n",
            "Epoch 50/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5047 - val_accuracy: 0.6667 - 79ms/epoch - 39ms/step\n",
            "Epoch 51/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5047 - val_accuracy: 0.6667 - 71ms/epoch - 35ms/step\n",
            "Epoch 52/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5048 - val_accuracy: 0.6667 - 72ms/epoch - 36ms/step\n",
            "Epoch 53/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5048 - val_accuracy: 0.6667 - 65ms/epoch - 33ms/step\n",
            "Epoch 54/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5048 - val_accuracy: 0.6667 - 54ms/epoch - 27ms/step\n",
            "Epoch 55/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 56/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 57/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 58/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 54ms/epoch - 27ms/step\n",
            "Epoch 59/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 59ms/epoch - 30ms/step\n",
            "Epoch 60/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 55ms/epoch - 27ms/step\n",
            "Epoch 61/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 54ms/epoch - 27ms/step\n",
            "Epoch 62/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5049 - val_accuracy: 0.6667 - 57ms/epoch - 29ms/step\n",
            "Epoch 63/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 51ms/epoch - 25ms/step\n",
            "Epoch 64/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 65/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 66/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 47ms/epoch - 24ms/step\n",
            "Epoch 67/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 68/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 73ms/epoch - 37ms/step\n",
            "Epoch 69/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 70/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 55ms/epoch - 27ms/step\n",
            "Epoch 71/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 72/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 51ms/epoch - 25ms/step\n",
            "Epoch 73/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 58ms/epoch - 29ms/step\n",
            "Epoch 74/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 51ms/epoch - 25ms/step\n",
            "Epoch 75/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 56ms/epoch - 28ms/step\n",
            "Epoch 76/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 51ms/epoch - 25ms/step\n",
            "Epoch 77/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 66ms/epoch - 33ms/step\n",
            "Epoch 78/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 50ms/epoch - 25ms/step\n",
            "Epoch 79/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 49ms/epoch - 24ms/step\n",
            "Epoch 80/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 57ms/epoch - 29ms/step\n",
            "Epoch 81/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 82/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 51ms/epoch - 25ms/step\n",
            "Epoch 83/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 84/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 55ms/epoch - 27ms/step\n",
            "Epoch 85/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 53ms/epoch - 27ms/step\n",
            "Epoch 86/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 47ms/epoch - 23ms/step\n",
            "Epoch 87/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 53ms/epoch - 26ms/step\n",
            "Epoch 88/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 53ms/epoch - 26ms/step\n",
            "Epoch 89/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 55ms/epoch - 27ms/step\n",
            "Epoch 90/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 91/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 49ms/epoch - 25ms/step\n",
            "Epoch 92/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 52ms/epoch - 26ms/step\n",
            "Epoch 93/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 48ms/epoch - 24ms/step\n",
            "Epoch 94/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 55ms/epoch - 27ms/step\n",
            "Epoch 95/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 61ms/epoch - 30ms/step\n",
            "Epoch 96/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 53ms/epoch - 27ms/step\n",
            "Epoch 97/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 45ms/epoch - 22ms/step\n",
            "Epoch 98/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 43ms/epoch - 21ms/step\n",
            "Epoch 99/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 53ms/epoch - 26ms/step\n",
            "Epoch 100/100\n",
            "2/2 - 0s - loss: 4.9671e-09 - accuracy: 1.0000 - val_loss: 1.5050 - val_accuracy: 0.6667 - 44ms/epoch - 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTMjzcNjvSqd",
        "outputId": "a1d176e1-fa5f-4208-b396-6e903fb91134"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step - loss: 1.5330 - accuracy: 0.4545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.533003330230713, 0.4545454680919647]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lf1tRzsmvdvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with one hidden layer"
      ],
      "metadata": {
        "id": "VCdYFGFriQSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Dense(64, activation='relu',input_dim=X_train.shape[1]))\n",
        "model4.add(Dense(32, activation='relu'))\n",
        "model4.add(Dense(2, activation='sigmoid')) #output layer\n",
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WBNe1TgvnOi",
        "outputId": "785f9f25-2677-4555-e5fd-5d3fa859cc33"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 64)                14016     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,162\n",
            "Trainable params: 16,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history=model4.fit(X_train,y_train,epochs=100,verbose=2,validation_split=0.2,batch_size=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckqq7CY3vnRh",
        "outputId": "4bd657a5-a0d4-4311-f4ed-33ffa5068868"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 - 1s - loss: 0.7015 - accuracy: 0.5000 - val_loss: 0.7065 - val_accuracy: 0.1667 - 564ms/epoch - 282ms/step\n",
            "Epoch 2/100\n",
            "2/2 - 0s - loss: 0.6886 - accuracy: 0.5833 - val_loss: 0.7071 - val_accuracy: 0.1667 - 30ms/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "2/2 - 0s - loss: 0.6783 - accuracy: 0.7083 - val_loss: 0.7077 - val_accuracy: 0.1667 - 39ms/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "2/2 - 0s - loss: 0.6684 - accuracy: 0.7917 - val_loss: 0.7084 - val_accuracy: 0.1667 - 43ms/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "2/2 - 0s - loss: 0.6597 - accuracy: 0.9167 - val_loss: 0.7092 - val_accuracy: 0.1667 - 34ms/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "2/2 - 0s - loss: 0.6504 - accuracy: 0.9167 - val_loss: 0.7101 - val_accuracy: 0.1667 - 28ms/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "2/2 - 0s - loss: 0.6423 - accuracy: 0.9583 - val_loss: 0.7113 - val_accuracy: 0.1667 - 42ms/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "2/2 - 0s - loss: 0.6333 - accuracy: 0.9583 - val_loss: 0.7125 - val_accuracy: 0.1667 - 52ms/epoch - 26ms/step\n",
            "Epoch 9/100\n",
            "2/2 - 0s - loss: 0.6247 - accuracy: 1.0000 - val_loss: 0.7140 - val_accuracy: 0.1667 - 32ms/epoch - 16ms/step\n",
            "Epoch 10/100\n",
            "2/2 - 0s - loss: 0.6158 - accuracy: 1.0000 - val_loss: 0.7158 - val_accuracy: 0.1667 - 40ms/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "2/2 - 0s - loss: 0.6059 - accuracy: 1.0000 - val_loss: 0.7177 - val_accuracy: 0.1667 - 33ms/epoch - 16ms/step\n",
            "Epoch 12/100\n",
            "2/2 - 0s - loss: 0.5959 - accuracy: 1.0000 - val_loss: 0.7200 - val_accuracy: 0.1667 - 34ms/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "2/2 - 0s - loss: 0.5852 - accuracy: 1.0000 - val_loss: 0.7225 - val_accuracy: 0.1667 - 29ms/epoch - 15ms/step\n",
            "Epoch 14/100\n",
            "2/2 - 0s - loss: 0.5737 - accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.1667 - 30ms/epoch - 15ms/step\n",
            "Epoch 15/100\n",
            "2/2 - 0s - loss: 0.5612 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.1667 - 35ms/epoch - 17ms/step\n",
            "Epoch 16/100\n",
            "2/2 - 0s - loss: 0.5480 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.1667 - 31ms/epoch - 16ms/step\n",
            "Epoch 17/100\n",
            "2/2 - 0s - loss: 0.5335 - accuracy: 1.0000 - val_loss: 0.7331 - val_accuracy: 0.1667 - 36ms/epoch - 18ms/step\n",
            "Epoch 18/100\n",
            "2/2 - 0s - loss: 0.5186 - accuracy: 1.0000 - val_loss: 0.7355 - val_accuracy: 0.3333 - 35ms/epoch - 17ms/step\n",
            "Epoch 19/100\n",
            "2/2 - 0s - loss: 0.5029 - accuracy: 1.0000 - val_loss: 0.7379 - val_accuracy: 0.3333 - 32ms/epoch - 16ms/step\n",
            "Epoch 20/100\n",
            "2/2 - 0s - loss: 0.4860 - accuracy: 1.0000 - val_loss: 0.7401 - val_accuracy: 0.3333 - 31ms/epoch - 15ms/step\n",
            "Epoch 21/100\n",
            "2/2 - 0s - loss: 0.4680 - accuracy: 1.0000 - val_loss: 0.7423 - val_accuracy: 0.3333 - 31ms/epoch - 15ms/step\n",
            "Epoch 22/100\n",
            "2/2 - 0s - loss: 0.4496 - accuracy: 1.0000 - val_loss: 0.7446 - val_accuracy: 0.3333 - 28ms/epoch - 14ms/step\n",
            "Epoch 23/100\n",
            "2/2 - 0s - loss: 0.4299 - accuracy: 1.0000 - val_loss: 0.7471 - val_accuracy: 0.3333 - 40ms/epoch - 20ms/step\n",
            "Epoch 24/100\n",
            "2/2 - 0s - loss: 0.4098 - accuracy: 1.0000 - val_loss: 0.7495 - val_accuracy: 0.3333 - 35ms/epoch - 18ms/step\n",
            "Epoch 25/100\n",
            "2/2 - 0s - loss: 0.3884 - accuracy: 1.0000 - val_loss: 0.7518 - val_accuracy: 0.3333 - 48ms/epoch - 24ms/step\n",
            "Epoch 26/100\n",
            "2/2 - 0s - loss: 0.3670 - accuracy: 1.0000 - val_loss: 0.7542 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n",
            "Epoch 27/100\n",
            "2/2 - 0s - loss: 0.3451 - accuracy: 1.0000 - val_loss: 0.7565 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "2/2 - 0s - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.7582 - val_accuracy: 0.3333 - 60ms/epoch - 30ms/step\n",
            "Epoch 29/100\n",
            "2/2 - 0s - loss: 0.3013 - accuracy: 1.0000 - val_loss: 0.7596 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "2/2 - 0s - loss: 0.2794 - accuracy: 1.0000 - val_loss: 0.7616 - val_accuracy: 0.3333 - 30ms/epoch - 15ms/step\n",
            "Epoch 31/100\n",
            "2/2 - 0s - loss: 0.2576 - accuracy: 1.0000 - val_loss: 0.7637 - val_accuracy: 0.3333 - 30ms/epoch - 15ms/step\n",
            "Epoch 32/100\n",
            "2/2 - 0s - loss: 0.2372 - accuracy: 1.0000 - val_loss: 0.7656 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "2/2 - 0s - loss: 0.2168 - accuracy: 1.0000 - val_loss: 0.7679 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n",
            "Epoch 34/100\n",
            "2/2 - 0s - loss: 0.1972 - accuracy: 1.0000 - val_loss: 0.7707 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 35/100\n",
            "2/2 - 0s - loss: 0.1788 - accuracy: 1.0000 - val_loss: 0.7735 - val_accuracy: 0.3333 - 31ms/epoch - 15ms/step\n",
            "Epoch 36/100\n",
            "2/2 - 0s - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.7762 - val_accuracy: 0.3333 - 39ms/epoch - 19ms/step\n",
            "Epoch 37/100\n",
            "2/2 - 0s - loss: 0.1445 - accuracy: 1.0000 - val_loss: 0.7789 - val_accuracy: 0.3333 - 33ms/epoch - 17ms/step\n",
            "Epoch 38/100\n",
            "2/2 - 0s - loss: 0.1296 - accuracy: 1.0000 - val_loss: 0.7815 - val_accuracy: 0.3333 - 31ms/epoch - 16ms/step\n",
            "Epoch 39/100\n",
            "2/2 - 0s - loss: 0.1153 - accuracy: 1.0000 - val_loss: 0.7843 - val_accuracy: 0.3333 - 35ms/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "2/2 - 0s - loss: 0.1031 - accuracy: 1.0000 - val_loss: 0.7875 - val_accuracy: 0.3333 - 33ms/epoch - 16ms/step\n",
            "Epoch 41/100\n",
            "2/2 - 0s - loss: 0.0921 - accuracy: 1.0000 - val_loss: 0.7907 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "2/2 - 0s - loss: 0.0817 - accuracy: 1.0000 - val_loss: 0.7940 - val_accuracy: 0.3333 - 41ms/epoch - 20ms/step\n",
            "Epoch 43/100\n",
            "2/2 - 0s - loss: 0.0726 - accuracy: 1.0000 - val_loss: 0.7972 - val_accuracy: 0.3333 - 34ms/epoch - 17ms/step\n",
            "Epoch 44/100\n",
            "2/2 - 0s - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.8000 - val_accuracy: 0.3333 - 32ms/epoch - 16ms/step\n",
            "Epoch 45/100\n",
            "2/2 - 0s - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.8025 - val_accuracy: 0.3333 - 33ms/epoch - 17ms/step\n",
            "Epoch 46/100\n",
            "2/2 - 0s - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.8049 - val_accuracy: 0.3333 - 36ms/epoch - 18ms/step\n",
            "Epoch 47/100\n",
            "2/2 - 0s - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.8072 - val_accuracy: 0.5000 - 33ms/epoch - 16ms/step\n",
            "Epoch 48/100\n",
            "2/2 - 0s - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.8094 - val_accuracy: 0.5000 - 46ms/epoch - 23ms/step\n",
            "Epoch 49/100\n",
            "2/2 - 0s - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.8115 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 50/100\n",
            "2/2 - 0s - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.8137 - val_accuracy: 0.5000 - 30ms/epoch - 15ms/step\n",
            "Epoch 51/100\n",
            "2/2 - 0s - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.8155 - val_accuracy: 0.5000 - 29ms/epoch - 15ms/step\n",
            "Epoch 52/100\n",
            "2/2 - 0s - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.5000 - 30ms/epoch - 15ms/step\n",
            "Epoch 53/100\n",
            "2/2 - 0s - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.8188 - val_accuracy: 0.5000 - 42ms/epoch - 21ms/step\n",
            "Epoch 54/100\n",
            "2/2 - 0s - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.8205 - val_accuracy: 0.5000 - 30ms/epoch - 15ms/step\n",
            "Epoch 55/100\n",
            "2/2 - 0s - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.8222 - val_accuracy: 0.5000 - 30ms/epoch - 15ms/step\n",
            "Epoch 56/100\n",
            "2/2 - 0s - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.8239 - val_accuracy: 0.5000 - 29ms/epoch - 14ms/step\n",
            "Epoch 57/100\n",
            "2/2 - 0s - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.8259 - val_accuracy: 0.5000 - 30ms/epoch - 15ms/step\n",
            "Epoch 58/100\n",
            "2/2 - 0s - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.8276 - val_accuracy: 0.5000 - 28ms/epoch - 14ms/step\n",
            "Epoch 59/100\n",
            "2/2 - 0s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.8294 - val_accuracy: 0.5000 - 33ms/epoch - 16ms/step\n",
            "Epoch 60/100\n",
            "2/2 - 0s - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.8310 - val_accuracy: 0.5000 - 40ms/epoch - 20ms/step\n",
            "Epoch 61/100\n",
            "2/2 - 0s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.8327 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 62/100\n",
            "2/2 - 0s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.8344 - val_accuracy: 0.5000 - 37ms/epoch - 19ms/step\n",
            "Epoch 63/100\n",
            "2/2 - 0s - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.8360 - val_accuracy: 0.5000 - 35ms/epoch - 17ms/step\n",
            "Epoch 64/100\n",
            "2/2 - 0s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.8376 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 65/100\n",
            "2/2 - 0s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.8391 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 66/100\n",
            "2/2 - 0s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.8404 - val_accuracy: 0.5000 - 39ms/epoch - 19ms/step\n",
            "Epoch 67/100\n",
            "2/2 - 0s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.8417 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 68/100\n",
            "2/2 - 0s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.8429 - val_accuracy: 0.5000 - 30ms/epoch - 15ms/step\n",
            "Epoch 69/100\n",
            "2/2 - 0s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.8441 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 70/100\n",
            "2/2 - 0s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.8452 - val_accuracy: 0.5000 - 33ms/epoch - 16ms/step\n",
            "Epoch 71/100\n",
            "2/2 - 0s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.8464 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 72/100\n",
            "2/2 - 0s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.8474 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "2/2 - 0s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.8484 - val_accuracy: 0.5000 - 32ms/epoch - 16ms/step\n",
            "Epoch 74/100\n",
            "2/2 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8493 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "2/2 - 0s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.8502 - val_accuracy: 0.5000 - 33ms/epoch - 16ms/step\n",
            "Epoch 76/100\n",
            "2/2 - 0s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.8510 - val_accuracy: 0.5000 - 37ms/epoch - 18ms/step\n",
            "Epoch 77/100\n",
            "2/2 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.8519 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 78/100\n",
            "2/2 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.8527 - val_accuracy: 0.5000 - 52ms/epoch - 26ms/step\n",
            "Epoch 79/100\n",
            "2/2 - 0s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.8534 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 80/100\n",
            "2/2 - 0s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.8542 - val_accuracy: 0.5000 - 37ms/epoch - 19ms/step\n",
            "Epoch 81/100\n",
            "2/2 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.8549 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "2/2 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.8556 - val_accuracy: 0.5000 - 36ms/epoch - 18ms/step\n",
            "Epoch 83/100\n",
            "2/2 - 0s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.8564 - val_accuracy: 0.5000 - 39ms/epoch - 19ms/step\n",
            "Epoch 84/100\n",
            "2/2 - 0s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.8572 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 85/100\n",
            "2/2 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.8580 - val_accuracy: 0.5000 - 38ms/epoch - 19ms/step\n",
            "Epoch 86/100\n",
            "2/2 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.8587 - val_accuracy: 0.5000 - 43ms/epoch - 22ms/step\n",
            "Epoch 87/100\n",
            "2/2 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.8593 - val_accuracy: 0.5000 - 41ms/epoch - 21ms/step\n",
            "Epoch 88/100\n",
            "2/2 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.8600 - val_accuracy: 0.5000 - 35ms/epoch - 17ms/step\n",
            "Epoch 89/100\n",
            "2/2 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.8606 - val_accuracy: 0.5000 - 35ms/epoch - 18ms/step\n",
            "Epoch 90/100\n",
            "2/2 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.8612 - val_accuracy: 0.5000 - 37ms/epoch - 18ms/step\n",
            "Epoch 91/100\n",
            "2/2 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.8618 - val_accuracy: 0.5000 - 34ms/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "2/2 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8624 - val_accuracy: 0.5000 - 32ms/epoch - 16ms/step\n",
            "Epoch 93/100\n",
            "2/2 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8631 - val_accuracy: 0.5000 - 39ms/epoch - 19ms/step\n",
            "Epoch 94/100\n",
            "2/2 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.8637 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 95/100\n",
            "2/2 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.8643 - val_accuracy: 0.5000 - 37ms/epoch - 18ms/step\n",
            "Epoch 96/100\n",
            "2/2 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.8649 - val_accuracy: 0.5000 - 31ms/epoch - 16ms/step\n",
            "Epoch 97/100\n",
            "2/2 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.8654 - val_accuracy: 0.5000 - 31ms/epoch - 15ms/step\n",
            "Epoch 98/100\n",
            "2/2 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8660 - val_accuracy: 0.5000 - 33ms/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "2/2 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8665 - val_accuracy: 0.5000 - 40ms/epoch - 20ms/step\n",
            "Epoch 100/100\n",
            "2/2 - 0s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8670 - val_accuracy: 0.5000 - 31ms/epoch - 15ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DTK88pqvtBp",
        "outputId": "156aae3a-d4e8-4263-df72-fc496c90b0ba"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step - loss: 0.8157 - accuracy: 0.5455\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8156866431236267, 0.5454545617103577]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "J-Yx1lRuvtE5",
        "outputId": "3362e4e8-f83f-4562-c5c9-91bc048666a3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xcdZ3/8de7aZv0RktvFNpiSoFCUa4BBHYFlHW5uHRxAamrtl5gZb2Ays8FFxVRf7v+ZFnXXRZFERBdCgKyxS1yE1BW0QYotwLSVqBBmoZAryFN0n5+f5yTOqQJnbRzMpk57+fjkUfnXGbmczIw75zv93y/RxGBmZnl15ByF2BmZuXlIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEFguSKqXFJKGFrHvfEkPDkRdZoOBg8AGHUnPS+qQNLHH+kfTL/P68lT2hlpGS9og6Y5y12K2sxwENlj9AZjbvSDpbcDI8pWzjb8BNgF/IWnKQL5xMWc1Zv3hILDB6nrgQwXL84AfFu4gaaykH0pqkfSCpIslDUm31Ui6TNIrklYAp/Ty3KslvSzpJUlfk1TTj/rmAd8BHgc+0OO1/0zSryWtkbRS0vx0/QhJ/5LWulbSg+m64yQ19XiN5yWdkD6+RNLNkn4kaR0wX9IRkn6TvsfLkv5D0vCC5x8g6W5Jr0pqlvQFSVMktUmaULDfoenvb1g/jt2qjIPABquHgF0k7Z9+QZ8F/KjHPv8OjAX2Ao4lCY4Pp9vOBt4DHAI0AKf3eO61QBewd7rPu4GPFVOYpLcAxwE/Tn8+1GPbHWltk4CDgSXp5suAw4CjgfHA54EtxbwnMAe4GRiXvudm4DPAROAo4F3A36c1jAHuAX4O7JEe470RsQq4Hziz4HU/CCyIiM4i67BqFBH+8c+g+gGeB04ALgb+CTgRuBsYCgRQD9QAHcDsguf9HXB/+vgXwMcLtr07fe5QYDeSZp0RBdvnAvelj+cDD75JfRcDS9LHU0m+lA9Jly8CftrLc4YArwMH9bLtOKCpt99B+vgS4Jfb+Z2d3/2+6bE82sd+7wP+N31cA6wCjij3Z+6f8v64rdEGs+uBXwIz6NEsRPKX8DDghYJ1L5B8MUPyl/DKHtu6vSV97suSutcN6bH/m/kQ8D2AiHhJ0gMkTUWPAtOB5b08ZyJQ18e2YryhNkn7ApeTnO2MJAm4h9PNfdUA8N/AdyTNAGYBayPidztYk1UJNw3ZoBURL5B0Gp8M3Npj8ytAJ8mXerc9gZfSxy+TfCEWbuu2kuSMYGJEjEt/domIA7ZXk6SjgX2AiyStkrQKOBJ4f9qJuxKY2ctTXwHa+9i2kYKO8LQpbFKPfXpOE3wl8AywT0TsAnwB6E61lSTNZduIiHbgJpJ+jQ+ShK3lnIPABruPAu+MiI2FKyNiM8kX2tcljUnb5j/Ln/oRbgI+LWmapF2BCwue+zJwF/AvknaRNETSTEnHFlHPPJJmqtkk7f8HA28FRgAnkbTfnyDpTElDJU2QdHBEbAF+AFwuaY+0M/soSbXA74E6SaeknbYXA7XbqWMMsA7YIGk/4NyCbT8Ddpd0vqTa9PdzZMH2H5I0f52Kg8BwENggFxHLI6Kxj82fIvlregXwIPBfJF+2kDTd3Ak8BjzCtmcUHwKGA0uB10g6Ynd/s1ok1ZF0tP57RKwq+PkDyRfqvIh4keQM5nPAqyQdxQelL3EB8ASwON32DWBIRKwl6ej9PskZzUbgDVcR9eIC4P3A+vRYb+zeEBHrgb8A/oqkD+A54PiC7f9L0kn9SHrWZTmnCN+YxixvJP0C+K+I+H65a7HycxCY5Yykw0mat6anZw+Wc24aMssRSdeRjDE43yFg3XxGYGaWcz4jMDPLuYobUDZx4sSor68vdxlmZhXl4YcffiUieo5PASowCOrr62ls7OtqQjMz642kPi8VdtOQmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlXGZBIOkHklZLerKP7ZL0bUnLJD0u6dCsajEzs75leUZwLcmdpfpyEsm87vsA55DMr25mZgMss3EEEfFLSfVvsssc4IeRzHHxkKRxknZP54qvSPc+3cxjK9eUuwwzq1Lv2n83Dpo+ruSvW84BZVN54+33mtJ12wSBpHNIzhrYc889e24eFNo7N/OpGx6lrWMzf7r7oZlZ6Uzepa7qgqBoEXEVcBVAQ0PDoJwl79fLX6GtYzPXfvhwjps1udzlmJkVrZxXDb3EG+8pO40/3W+24ty9tJnRtUM5auaEcpdiZtYv5QyChcCH0quH3g6srdT+gS1bgruXrubYWZOoHVpT7nLMzPols6YhSTcAxwETJTUBXwaGAUTEd4BFJPd2XQa0AR/OqpasPbpyDa9s2MS7Z+9W7lLMzPoty6uG5m5newCfyOr9B9JdS1cxdIjcN2BmFckji0vg7qeaOWrmBMaOGFbuUszM+s1BsJOWrd7Ailc28hduFjKzCuUg2El3LV0FwAn7OwjMrDJVxDiCwSQi+PdfLOPFV9sA+M3yVg6cNpY9xo0oc2VmZjvGQdBPy1Zv4PK7f8+EUcOpG5ZcKjrvqPryFmVmthMcBP1019JmAP7n03/OlLF1Za7GzGznuY+gn+5a2sxB08Y6BMysajgI+qF5XTuPrVzDuw+YUu5SzMxKxkHQD3enzUIeQWxm1cRB0A93LW2mfsJI9p48utylmJmVjIOgSOvaO/nN8ld49wFTkG84YGZVxEFQpAeebaFzc7hZyMyqjoOgSHctbWbCqOEcsueu5S7FzKykPI6gD+2dm/nsTUtY09YJwCMvvsacg6ZSM8TNQmZWXXxG0IelL69j0ROraN3QQefmLRw8fRwfPOot5S7LzKzkfEbQh9Xr2gG4/H0HccAeY8tcjZlZdnxG0IdVa5Mg2G0XjyA2s+rmIOhD8/pNDKsR40cOL3cpZmaZchD0oXltO5PH1DHEncNmVuUcBH1oXt/ObrvUlrsMM7PMOQj6sGptu/sHzCwXHAR9WL1uk4PAzHLBQdCLjZu6WL+py0FgZrngIOjFqnQMwZSx7iMws+rnIOhF8zqPITCz/HAQ9MJBYGZ54iDoRfO6TYCDwMzywUHQi1Vr2xldO5TRtZ6Kycyqn4OgF6s9mMzMcsRB0AsPJjOzPHEQ9KJ53SamOAjMLCccBD1s2RKsXt/OZAeBmeWEg6CH19o66NwcTHEfgZnlhIOgh1UeQ2BmOeMg6GF19xiCsQ4CM8sHB0EPPiMws7xxEPTQPb3E5DHuIzCzfHAQ9NC8rp2Jo4czrMa/GjPLB3/b9dDsG9KYWc5kGgSSTpT0rKRlki7sZfueku6T9KikxyWdnGU9xfCoYjPLm8yCQFINcAVwEjAbmCtpdo/dLgZuiohDgLOA/8yqnmIl8ww5CMwsP7I8IzgCWBYRKyKiA1gAzOmxTwC7pI/HAn/MsJ7t6ujawisbOjzhnJnlSpZBMBVYWbDclK4rdAnwAUlNwCLgU729kKRzJDVKamxpacmiVgBaNiRjCDzPkJnlSbk7i+cC10bENOBk4HpJ29QUEVdFRENENEyaNCmzYlatfR3wGAIzy5csg+AlYHrB8rR0XaGPAjcBRMRvgDpgYoY1vakVLRsBqJ84qlwlmJkNuCyDYDGwj6QZkoaTdAYv7LHPi8C7ACTtTxIE2bX9bMfylo0MqxHTdx1RrhLMzAZcZkEQEV3AJ4E7gadJrg56StKlkk5Nd/sccLakx4AbgPkREVnVtD0rWjZQP2EUQz2YzMxyJNOb8kbEIpJO4MJ1Xyp4vBQ4Jssa+mN5ywb2njy63GWYmQ0o/+mb6ty8hRda25g5yUFgZvniIEi9+GobXVvCQWBmueMgSC1fvQGAmW4aMrOccRCkVrySXDq61yRfOmpm+eIgSC1fvYFJY2rZpW5YuUsxMxtQDoLU8pYNzPTZgJnlkIMAiAiWt2x0R7GZ5ZKDAGjd2MHa1zsdBGaWSw4CfMWQmeWbg4CCK4Y82ZyZ5ZCDgOSMoHboEKaO82RzZpY/DgKSK4b2mjSaIUNU7lLMzAacgwDSK4bcLGRm+ZT7IGjv3MzK1zzZnJnlV+6D4IXWNiJ8xZCZ5Vfug+AP6RVDMya4acjM8in3QdB9w/rdx/mG9WaWT7kPgub1mxhWI8aPHF7uUszMysJBsLadyWPqfOmomeWWg2B9O7vtUlvuMszMyib3QbBqbTu77eL+ATPLr9wHwep1mxwEZpZruQ6CjZu6WL+py0FgZrmW6yBYta4dgClj3UdgZvmV6yBoToNgtzE+IzCz/CoqCCTdKukUSVUVHFuDYKyDwMzyq9gv9v8E3g88J+mfJc3KsKYB07xuE4D7CMws14oKgoi4JyL+FjgUeB64R9KvJX1Y0rAsC8zSqrXtjK4dyujaoeUuxcysbIpu6pE0AZgPfAx4FPg3kmC4O5PKBsBqDyYzM6OoP4Ul/RSYBVwP/FVEvJxuulFSY1bFZc2DyczMigwC4NsRcV9vGyKioYT1DKjmdZs4csb4cpdhZlZWxTYNzZY0rntB0q6S/j6jmgbEli3B6vXtTPYZgZnlXLFBcHZErOleiIjXgLOzKWlgvNbWQefmYIr7CMws54oNghpJW+dpllQDVPQE/t2jit1HYGZ5V2wfwc9JOoa/my7/XbquYnkwmZlZotgg+AeSL/9z0+W7ge9nUtEA8WAyM7NEUUEQEVuAK9OfqrBqbXJGMHmM+wjMLN+KHUewD/BPwGxg65/QEbFXRnVlbvX6diaOHs6wmqqaPsnMrN+K/Ra8huRsoAs4Hvgh8KPtPUnSiZKelbRM0oV97HOmpKWSnpL0X8UWvrM8mMzMLFFsEIyIiHsBRcQLEXEJcMqbPSG9sugK4CSSM4m5kmb32Gcf4CLgmIg4ADi/n/XvsOZ1m5jiIDAzKzoINqVTUD8n6ZOSTgNGb+c5RwDLImJFRHQAC4A5PfY5G7giHZdARKzuR+07pXmdB5OZmUHxQXAeMBL4NHAY8AFg3naeMxVYWbDclK4rtC+wr6T/lfSQpBN7eyFJ50hqlNTY0tJSZMl96+jaQuvGDp8RmJlRRGdx2sTzvoi4ANgAfLjE778PcBwwDfilpLcVjmIGiIirgKsAGhoaYmffdPX67sFkvmLIzGy7ZwQRsRn4sx147ZeA6QXL09J1hZqAhRHRGRF/AH5PEgyZ2jqGwIPJzMyKHlD2qKSFwE+Ajd0rI+LWN3nOYmAfSTNIAuAskrucFboNmAtcI2kiSVPRiiJr2mG+V7GZ2Z8UGwR1QCvwzoJ1AfQZBBHRJemTwJ1ADfCDiHhK0qVAY0QsTLe9W9JSYDPwfyKidQeOo19WvtoGwNRdR2T9VmZmg16xI4t3qF8gIhYBi3qs+1LB4wA+m/4MmOUtG5g4upaxIyr2LptmZiVT7Mjia0jOAN4gIj5S8ooGwPKWjcycNKrcZZiZDQrFNg39rOBxHXAa8MfSlzMwlrds4OS37V7uMszMBoVim4ZuKVyWdAPwYCYVZezVjR2saetk5qTtjYczM8uHHZ1xbR9gcikLGSjLWzYAuGnIzCxVbB/Bet7YR7CK5B4FFWf56u4g8BmBmRkU3zQ0JutCBsrylg3UDh3CHuN86aiZGRTZNCTpNEljC5bHSfrr7MrKzvKWjcyYOIqaIdr+zmZmOVBsH8GXI2Jt90I6F9CXsykpW8tbNjBzspuFzMy6FRsEve1X7KWng8amrs2sfLXN/QNmZgWKDYJGSZdLmpn+XA48nGVhWXihtY0t4SuGzMwKFRsEnwI6gBtJbjDTDnwiq6Ky4iuGzMy2VexVQxuBXu85XEm6xxDs5TMCM7Otir1q6G5J4wqWd5V0Z3ZlZWN5y0b2GFvHyOEV171hZpaZYpuGJhbeNSy9x3DFjSz2FUNmZtsqNgi2SNqze0FSPb3MRjqYRQQrWja6f8DMrIdi20j+EXhQ0gOAgD8HzsmsqgysXr+JDZu6fMWQmVkPxXYW/1xSA8mX/6Mkt5h8PcvCSs1XDJmZ9a7YSec+BpxHcgP6JcDbgd/wxltXDmp/umLIQWBF2rIZrjkJXnu+3JWYJU74Chw8t+QvW2zT0HnA4cBDEXG8pP2A/1vyajL0lgmjOOOwaey2S225S7FK0dYKK38Lex4Nk/YtdzVmMG56Ji9bbBC0R0S7JCTVRsQzkmZlUlFG3rHvJN6x76Ryl2GVpK01+feIj8Fb/6a8tZhlqNggaErHEdwG3C3pNeCF7MoyGwS6g2DkhPLWYZaxYjuLT0sfXiLpPmAs8PPMqjIbDNpeTf51EFiV6/cQ24h4IItCzAYdnxFYTuzoPYvNql93EIwYX946zDLmIDDrS9urMHw0DKsrdyVmmXIQmPWlrRVG+mzAqp+DwKwvba3uH7BccBCY9cVBYDnhIDDri4PAcsJBYNaXtlcdBJYLDgKz3nRtgo717iy2XHAQmPXGo4otRxwEZr3xYDLLEQeBWW88vYTliIPArDcOAssRB4FZb153H4Hlh4PArDdbO4vdR2DVz0Fg1pu2VqgdCzXDyl2JWeYyDQJJJ0p6VtIySRe+yX5/IykkNWRZj1nRPOGc5UhmQSCpBrgCOAmYDcyVNLuX/cYA5wG/zaoWs37z9BKWI1meERwBLIuIFRHRASwA5vSy31eBbwDtGdZi1j8OAsuRLINgKrCyYLkpXbeVpEOB6RHxPxnWYdZ/nmfIcqRsncWShgCXA58rYt9zJDVKamxpacm+ODP3EViOZBkELwHTC5anpeu6jQHeCtwv6Xng7cDC3jqMI+KqiGiIiIZJkyZlWLIZ0NEGnW0+I7DcyDIIFgP7SJohaThwFrCwe2NErI2IiRFRHxH1wEPAqRHRmGFNZtvnwWSWM5kFQUR0AZ8E7gSeBm6KiKckXSrp1Kze12yneXoJy5mhWb54RCwCFvVY96U+9j0uy1rMirY1CNxHYPngkcVmPfleBJYzDgKznhwEljMOArOe2loBQd24cldiNiAcBGY9tbXCiHFQk2kXmtmg4SAw68nTS1jOOAjMenIQWM44CMx68jxDljMOArOePM+Q5YyDwKxQhJuGLHd8WUSlioBn74D2teWupLps7oDNmxwElisOgkq16glYMLfcVVSv8XuVuwKzAeMgqFQbmpN/z7weprytvLVUm5rhMHbq9vczqxIOgkrVPTHabgfA+BnlrcXMKpo7iyuVp0o2sxJxEFSqtlZQDdSNLXclZlbhHASVqvtad6nclZhZhXMQVCqPfjWzEnEQVCoHgZmViIOgUnkaBDMrEQdBpfI0CGZWIg6CSuT5cMyshBwElah9LcRmB4GZlYRHFlciDyazKtHZ2UlTUxPt7e3lLqVq1NXVMW3aNIYNG1b0cxwElajt1eRfB4FVuKamJsaMGUN9fT3ymJidFhG0trbS1NTEjBnFTz3jpqFK1H1GMMJXDVlla29vZ8KECQ6BEpHEhAkT+n2G5SCoRFubhhwEVvkcAqW1I79PB0Elet1NQ2ZWOg6CStTWCkOGQe2YcldiVtFaW1s5+OCDOfjgg5kyZQpTp07dutzR0fGmz21sbOTTn/70AFWaLXcWV6LuMQQ+pTbbKRMmTGDJkiUAXHLJJYwePZoLLrhg6/auri6GDu39a7KhoYGGhoYBqTNrDoJK5HmGrAp95fanWPrHdSV9zdl77MKX/+qAfj1n/vz51NXV8eijj3LMMcdw1llncd5559He3s6IESO45pprmDVrFvfffz+XXXYZP/vZz7jkkkt48cUXWbFiBS+++CLnn39+RZ0tOAgqkecZMstUU1MTv/71r6mpqWHdunX86le/YujQodxzzz184Qtf4JZbbtnmOc888wz33Xcf69evZ9asWZx77rn9upa/nBwElaitFSbPLncVZiXV37/cs3TGGWdQU1MDwNq1a5k3bx7PPfcckujs7Oz1Oaeccgq1tbXU1tYyefJkmpubmTZt2kCWvcPcWVyJPM+QWaZGjRq19fEXv/hFjj/+eJ588kluv/32Pq/Rr62t3fq4pqaGrq6uzOssFQdBpdmyGV5/zUFgNkDWrl3L1KlTAbj22mvLW0xGHASVpn0txBYHgdkA+fznP89FF13EIYccUlF/5feHIqLcNfRLQ0NDNDY2lruM8nnlOfiPBnjv9+DAM8tdjdlOefrpp9l///3LXUbV6e33KunhiOj1elefEVSarRPO+aohMysNB0Gl8RTUZlZiDoJK4yAwsxLLNAgknSjpWUnLJF3Yy/bPSloq6XFJ90p6S5b1VAUHgZmVWGZBIKkGuAI4CZgNzJXUcxTUo0BDRBwI3Az8v6zqqRptrTC0DoaNLHclZlYlsjwjOAJYFhErIqIDWADMKdwhIu6LiLZ08SGgMobhlVP3PEOecM7MSiTLIJgKrCxYbkrX9eWjwB29bZB0jqRGSY0tLS0lLLECeZ4hs5I5/vjjufPOO9+w7lvf+hbnnntur/sfd9xxdF++fvLJJ7NmzZpt9rnkkku47LLL3vR9b7vtNpYuXbp1+Utf+hL33HNPf8svmUHRWSzpA0AD8M3etkfEVRHREBENkyZNGtjiBhtPL2FWMnPnzmXBggVvWLdgwQLmzp273ecuWrSIcePG7dD79gyCSy+9lBNOOGGHXqsUspx07iVgesHytHTdG0g6AfhH4NiI2JRhPdWhrRXGTd/+fmaV5o4LYdUTpX3NKW+Dk/65z82nn346F198MR0dHQwfPpznn3+eP/7xj9xwww189rOf5fXXX+f000/nK1/5yjbPra+vp7GxkYkTJ/L1r3+d6667jsmTJzN9+nQOO+wwAL73ve9x1VVX0dHRwd57783111/PkiVLWLhwIQ888ABf+9rXuOWWW/jqV7/Ke97zHk4//XTuvfdeLrjgArq6ujj88MO58sorqa2tpb6+nnnz5nH77bfT2dnJT37yE/bbb7+S/JqyPCNYDOwjaYak4cBZwMLCHSQdAnwXODUiVmdYS/Voa/VN681KZPz48RxxxBHccUfSKr1gwQLOPPNMvv71r9PY2Mjjjz/OAw88wOOPP97nazz88MMsWLCAJUuWsGjRIhYvXrx123vf+14WL17MY489xv7778/VV1/N0Ucfzamnnso3v/lNlixZwsyZM7fu397ezvz587nxxht54okn6Orq4sorr9y6feLEiTzyyCOce+65221+6o/MzggiokvSJ4E7gRrgBxHxlKRLgcaIWEjSFDQa+El6w+UXI+LUrGqqeJu7krmG3DRk1ehN/nLPUnfz0Jw5c1iwYAFXX301N910E1dddRVdXV28/PLLLF26lAMPPLDX5//qV7/itNNOY+TI5Eq+U0/901fYk08+ycUXX8yaNWvYsGEDf/mXf/mmtTz77LPMmDGDfffdF4B58+ZxxRVXcP755wNJsAAcdthh3HrrrTt97N0yvR9BRCwCFvVY96WCx+VrFKtE7WuAcBCYldCcOXP4zGc+wyOPPEJbWxvjx4/nsssuY/Hixey6667Mnz+/z6mnt2f+/PncdtttHHTQQVx77bXcf//9O1Vr91TXpZ7melB0FluRtg4mc9OQWamMHj2a448/no985CPMnTuXdevWMWrUKMaOHUtzc/PWZqO+vOMd7+C2227j9ddfZ/369dx+++1bt61fv57dd9+dzs5OfvzjH29dP2bMGNavX7/Na82aNYvnn3+eZcuWAXD99ddz7LHHluhI+5afO5Q9cj385j/KXcXO6Xw9+ddBYFZSc+fO5bTTTmPBggXst99+HHLIIey3335Mnz6dY4455k2fe+ihh/K+972Pgw46iMmTJ3P44Ydv3fbVr36VI488kkmTJnHkkUdu/fI/66yzOPvss/n2t7/NzTffvHX/uro6rrnmGs4444ytncUf//jHsznoAvmZhvqZ/4HHbyx9QQNt+JikLbV2TLkrMdtpnoY6G/2dhjo/ZwT7nZL8mJnZG7iPwMws5xwEZlZWldY8PdjtyO/TQWBmZVNXV0dra6vDoEQigtbWVurq6vr1vPz0EZjZoDNt2jSamprI/WSSJVRXV8e0af2byNlBYGZlM2zYMGbMmFHuMnLPTUNmZjnnIDAzyzkHgZlZzlXcyGJJLcALO/j0icArJSynUuTxuPN4zJDP487jMUP/j/stEdHrnb0qLgh2hqTGvoZYV7M8Hncejxnyedx5PGYo7XG7acjMLOccBGZmOZe3ILiq3AWUSR6PO4/HDPk87jweM5TwuHPVR2BmZtvK2xmBmZn14CAwM8u53ASBpBMlPStpmaQLy11PFiRNl3SfpKWSnpJ0Xrp+vKS7JT2X/rtruWstNUk1kh6V9LN0eYak36af942Shpe7xlKTNE7SzZKekfS0pKNy8ll/Jv3v+0lJN0iqq7bPW9IPJK2W9GTBul4/WyW+nR7745IO7e/75SIIJNUAVwAnAbOBuZJml7eqTHQBn4uI2cDbgU+kx3khcG9E7APcmy5Xm/OApwuWvwH8a0TsDbwGfLQsVWXr34CfR8R+wEEkx1/Vn7WkqcCngYaIeCtQA5xF9X3e1wIn9ljX12d7ErBP+nMOcGV/3ywXQQAcASyLiBUR0QEsAOaUuaaSi4iXI+KR9PF6ki+GqSTHel2623XAX5enwmxImgacAnw/XRbwTqD7ruDVeMxjgXcAVwNEREdErKHKP+vUUGCEpKHASOBlquzzjohfAq/2WN3XZzsH+GEkHgLGSdq9P++XlyCYCqwsWG5K11UtSfXAIcBvgd0i4uV00ypgtzKVlZVvAZ8HtqTLE4A1EdGVLlfj5z0DaAGuSZvEvi9pFFX+WUfES8BlwIskAbAWeJjq/7yh7892p7/f8hIEuSJpNHALcH5ErCvcFsn1wlVzzbCk9wCrI+LhctcywIYChwJXRsQhwEZ6NANV22cNkLaLzyEJwj2AUWzbhFL1Sv3Z5iUIXgKmFyxPS9dVHUnDSELgxxFxa7q6uftUMf13dbnqy8AxwKmSnidp8nsnSdv5uLTpAKrz824CmiLit+nyzSTBUM2fNcAJwB8ioiUiOoFbSf4bqPbPG/r+bHf6+y0vQbAY2Ce9smA4SefSwjLXVHJp2/jVwNMRcXnBpoXAvPTxPOC/B7q2rETERYdl3jgAAAKESURBVBExLSLqST7XX0TE3wL3Aaenu1XVMQNExCpgpaRZ6ap3AUup4s869SLwdkkj0//eu4+7qj/vVF+f7ULgQ+nVQ28H1hY0IRUnInLxA5wM/B5YDvxjuevJ6Bj/jOR08XFgSfpzMkmb+b3Ac8A9wPhy15rR8R8H/Cx9vBfwO2AZ8BOgttz1ZXC8BwON6ed9G7BrHj5r4CvAM8CTwPVAbbV93sANJH0gnSRnfx/t67MFRHJV5HLgCZIrqvr1fp5iwsws5/LSNGRmZn1wEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4HZAJJ0XPcMqWaDhYPAzCznHARmvZD0AUm/k7RE0nfT+x1skPSv6Vz490qalO57sKSH0rngf1owT/zeku6R9JikRyTNTF9+dMF9BH6cjpA1KxsHgVkPkvYH3gccExEHA5uBvyWZ4KwxIg4AHgC+nD7lh8A/RMSBJCM7u9f/GLgiIg4CjiYZKQrJrLDnk9wbYy+SuXLMymbo9ncxy513AYcBi9M/1keQTPC1Bbgx3edHwK3pfQHGRcQD6frrgJ9IGgNMjYifAkREO0D6er+LiKZ0eQlQDzyY/WGZ9c5BYLYtAddFxEVvWCl9scd+Ozo/y6aCx5vx/4dWZm4aMtvWvcDpkibD1nvFvoXk/5fuGS7fDzwYEWuB1yT9ebr+g8ADkdwhrknSX6evUStp5IAehVmR/JeIWQ8RsVTSxcBdkoaQzAD5CZKbvxyRbltN0o8AyZTA30m/6FcAH07XfxD4rqRL09c4YwAPw6xonn3UrEiSNkTE6HLXYVZqbhoyM8s5nxGYmeWczwjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCzn/j8ayA6HNkvLYgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w8a9BGaMwFBX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}